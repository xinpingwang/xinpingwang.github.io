---
title: ç¬¬ä¸€è¡Œä»£ç 
date: 2019-10-22
tags:
 - machine learning framework
---

ä»Šå¤©ç»ˆäºè¦æ­£å¼å¼€å§‹æ„å»ºæœºå™¨å­¦ä¹ æ¡†æ¶äº†ğŸ‘ï¼Œåœ¨ç›®å‰æœ€ä¸ºä¸»æµçš„ç¥ç»ç½‘ç»œæ¡†æ¶ä¸­ TensorFlow æœ‰ 300 å¤šä¸‡è¡Œä»£ç ï¼ŒPyTorch çš„ä»£ç é‡ç›¸å¯¹å°‘ä¸€äº›ä½†ä¹Ÿæœ‰ 80 å¤šä¸‡è¡Œï¼Œæ˜¾ç„¶ä»¥ä¸ªäººèƒ½åŠ›ä¸å¯èƒ½æ„å»ºå‡ºä¸€ä¸ªå¦‚æ­¤å¤æ‚çš„å·¥ç¨‹ï¼Œç”šè‡³ä»¥ç¬”è€…ç›®å‰çš„å·¥ç¨‹èƒ½åŠ›å’Œå­¦æœ¯æ°´å¹³éƒ½ä¸è¶³ä»¥å®Œæˆä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œæ¡†æ¶çš„è®¾è®¡ä¸æ„å»ºã€‚å› æ­¤ï¼Œæˆ‘é€‰æ‹©äº†ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„ç¥ç»ç½‘ç»œæ¡†æ¶ [decaf](https://github.com/Yangqing/decaf.git) ~~ä½œ~~ï¼ˆjingï¼‰~~ä¸º~~ï¼ˆxingï¼‰~~å‚~~ï¼ˆchaoï¼‰~~è€ƒ~~ï¼ˆxiï¼‰ï¼Œè¯¥é¡¹ç›®æ˜¯è´¾æ‰¬æ¸…åœ¨ 2013 å¹´ï¼ˆ7 æœˆâ€”â€”9 æœˆï¼‰å®Œæˆçš„ä¸€ä¸ªé¡¹ç›®ï¼Œæœ€ç»ˆç‰ˆæœ¬ä»£ç é‡åœ¨ 1 ä¸‡è¡Œå·¦å³ï¼Œæ ¹æ® GitHub ä¸Šçš„é¡¹ç›®ç®€ä»‹ï¼Œè¯¥é¡¹ç›®æ˜¯ Caffe çš„å‰èº«ï¼Œç›®æ ‡æ˜¯å®ç°ä¸€ä¸ªé«˜æ•ˆè€Œçµæ´»çš„å·ç§¯ç¥ç»ç½‘ç»œæ¡†æ¶ã€‚

<escape><!-- more --></escape>

## ä¸€ä¸ªä¾‹å­

åœ¨å¼€å§‹å…·ä½“çš„è®¾è®¡ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹ä½¿ç”¨ç°åœ¨ä¸»æµçš„ç¥ç»ç½‘ç»œæ¡†æ¶ä¹‹ä¸€ â€”â€” PyTorch æ—¶ï¼Œæ˜¯å¦‚ä½•å®ç°ä¸€ä¸ªç½‘ç»œçš„ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
```

è¿™æ®µä»£ç æ¥è‡ª PyTorch åŸºç¡€æ•™ç¨‹ [Neural Networks Tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)ï¼Œå®ç°äº†è‘—åçš„ **LeNet**ï¼Œå¯ä»¥è¯´éå¸¸çš„ Pythonã€‚æˆ‘ä»¬ä»æœºå™¨å­¦ä¹ æ¶è®¾è®¡çš„è§’åº¦æ¥çœ‹ä¸€ä¸‹è¿™æ®µä»£ç ï¼š

1. åœ¨ `__init__` å‡½æ•°é‡Œï¼Œç”¨åˆ°äº† `nn.Linear` å’Œ `nn.Conv2d`ï¼Œè¿™ä¸¤ä¸ªç½‘ç»œå±‚æ˜¯ç”± PyTorch ä¸ºæˆ‘ä»¬æä¾›çš„ï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦å®ç°ä¸€äº›å¸¸è§çš„ç½‘ç»œå±‚ï¼Œä¾›ç”¨æˆ·ä½¿ç”¨ï¼›
2. å®šä¹‰äº† `forward` å‡½æ•°æ¥å¤„ç†æ­£å‘ä¼ æ’­çš„è®¡ç®—ï¼Œä½†æ˜¯ï¼Œå´æ²¡æœ‰å¤„ç†å¦‚ä½•è¿›è¡Œåå‘ä¼ æ’­ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ¡†æ¶è¦è‡ªåŠ¨æ ¹æ®ç”¨æˆ·å®šä¹‰çš„æ­£å‘ä¼ æ’­è®¡ç®—è¿‡ç¨‹ï¼Œæ¥æ„å»ºè®¡ç®—å›¾ï¼Œå¹¶æ ¹æ®è®¡ç®—å›¾å®ç°åå‘ä¼ æ’­ã€‚

## ä¸‰ä¸ªç›®æ ‡

é€šè¿‡ä¸Šé¢ä¸¤ç‚¹ï¼Œæˆ‘ä»¬åŸºæœ¬å¯ä»¥æ˜ç¡®æˆ‘ä»¬åœ¨æœ¬æœºå™¨å­¦ä¹ æ¡†æ¶ä¸­éœ€è¦å®Œæˆçš„äº‹æƒ…ï¼š

1. å®ç°ä¸€äº›å¸¸è§çš„ç½‘ç»œå±‚ï¼Œä¾‹å¦‚ï¼šLinearã€Conv2dï¼Œä»¥åŠä¸€äº›å¸¸ç”¨çš„æ¿€æ´»å‡½æ•° ReLUã€Sigmoidï¼ŒåŒæ—¶ï¼Œè¿˜è¦æä¾›ç›¸åº”çš„åå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨æ±‚å¯¼ï¼‰è®¡ç®—å®ç°ï¼›
2. å®ç°è®¡ç®—å›¾çš„æ„å»ºï¼›
3. å®ç°ä¸€äº›å¸¸ç”¨çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–ç®—æ³•ã€‚

## æ¡†æ¶è®¾è®¡

é’ˆå¯¹è¿™å‡ ä¸ªç›®æ ‡ï¼Œéœ€è¦æŠ½è±¡å‡ºä¸€äº›æ•°æ®ç»“æ„å’Œæ¥å£ã€‚ä¸»è¦æœ‰ Blobã€Layer å’Œ Netã€‚

### Blob

æ ¹æ® [å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­](https://xinpingwang.github.io/2019/10/15/forward-and-backward/) ä¸­çš„æè¿°ï¼Œåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åŒæ—¶ä¿ç•™å‚æ•°çš„å½“å‰å€¼å’Œæ¢¯åº¦ã€‚æˆ‘ä»¬å°†è¿™ä¸¤ä¸ªæ•°æ®æ”¾åœ¨ä¸€èµ·ç»„æˆä¸€ä¸ª `Blob`ï¼š

```python
class Blob(object):
    
    def __init__(self, shape=None, dtype=None):
        if shape is None and dtype is None:
            self._data = None
        else:
            self._data = np.zeros(shape, dtype=dtype)
        self._diff = None
```

å…¶ä¸­ `_data` å’Œ `_diff` éƒ½æ˜¯ NdArrayã€‚

### Layer

`Layer` æ˜¯å¯¹ä¸åŒç½‘ç»œå±‚çš„æŠ½è±¡ï¼Œå®šä¹‰äº† `forward` å’Œ `backward` å‡½æ•°ï¼Œéœ€è¦åœ¨å­ç±»ä¸­è¿›è¡Œå®ç°ã€‚æ¨¡å‹è¢«çœ‹æˆä¸€ä¸ªä»ä¸‹å¾€ä¸Šç®—çš„è¿‡ç¨‹ï¼Œæ‰€ä»¥ä½¿ç”¨ `bottom` å’Œ `top` æ¥ä»£è¡¨æ¯ä¸€ä¸ª Layer çš„è¾“å…¥å’Œè¾“å‡ºã€‚

```python
class Layer(object):
    
    def __init__(self, **kwargs):
        self.spec = kwargs
        self.name = self.spec['name']
        self._param = []

    def forward(self, bottom, top):
        raise NotImplementedError

    def backward(self, bottom, top, propagate_down):
        raise NotImplementedError

    def update(self):
        raise NotImplementedError

    def param(self):
        return self._param
```

### Net

`Net` è¡¨ç¤ºä¸€ä¸ªå…·ä½“çš„æ¨¡å‹ï¼Œç”±ä¸€ä¸ªæˆ–å¤šä¸ª `Layer` ç»„æˆï¼Œå¹¶æ ¹æ® `Layer` æ¥æ„å»ºå‡ºè®¡ç®—å›¾ï¼Œå®ç°ç›¸å¯¹å¤æ‚ä¸€äº›ï¼Œåé¢ä¼šè½¬é—¨å†™ä¸€èŠ‚æ¥ä»‹ç» Net çš„å®ç°ã€‚

```python
class Net(object):

    def __init__(self):
        self._graph = nx.DiGraph()
        self._blobs = defaultdict(Blob)
        self._layers = {}
        self._needs = {}
        self._provides = {}
        # The topological order to execute the layer.
        self._forward_order = None
        self._backward_order = None

    def add_layer(self, layer, needs=[], provides=[]):
        pass

    def execute(self):
        # the forward pass. we will also accumulate the loss function
        loss = 0.
        for _, layer, bottom, top in self._forward_order:
            loss += layer.forward(bottom, top)
        # the backward pass
        for _, layer, bottom, top, propagate_down in self._backward_order:
            layer.backward(bottom, top, propagate_down)
        return loss

    def update(self):
        for _, layer, _, _ in self._forward_order:
            layer.update()
```





